{
  "results": {
    "mmlu-abstract_algebra": {
      "acc": 0.21,
      "num_samples": 100,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "mmlu-anatomy": {
      "acc": 0.3037037037037037,
      "num_samples": 135,
      "acc_stderr": 0.039725528847851375,
      "acc_norm": 0.22962962962962963,
      "acc_norm_stderr": 0.03633384414073464
    },
    "mmlu-astronomy": {
      "acc": 0.3815789473684211,
      "num_samples": 152,
      "acc_stderr": 0.03953173377749193,
      "acc_norm": 0.40131578947368424,
      "acc_norm_stderr": 0.03988903703336285
    },
    "mmlu-business_ethics": {
      "acc": 0.51,
      "num_samples": 100,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "mmlu-clinical_knowledge": {
      "acc": 0.3622641509433962,
      "num_samples": 265,
      "acc_stderr": 0.0295822451283843,
      "acc_norm": 0.39245283018867927,
      "acc_norm_stderr": 0.03005258057955784
    },
    "mmlu-college_biology": {
      "acc": 0.3541666666666667,
      "num_samples": 144,
      "acc_stderr": 0.039994111357535424,
      "acc_norm": 0.2638888888888889,
      "acc_norm_stderr": 0.03685651095897532
    },
    "mmlu-college_chemistry": {
      "acc": 0.2,
      "num_samples": 100,
      "acc_stderr": 0.04020151261036846,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "mmlu-college_computer_science": {
      "acc": 0.32,
      "num_samples": 100,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "mmlu-college_mathematics": {
      "acc": 0.21,
      "num_samples": 100,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "mmlu-college_medicine": {
      "acc": 0.3236994219653179,
      "num_samples": 173,
      "acc_stderr": 0.03567603799639172,
      "acc_norm": 0.28901734104046245,
      "acc_norm_stderr": 0.034564257450869995
    },
    "mmlu-college_physics": {
      "acc": 0.27450980392156865,
      "num_samples": 102,
      "acc_stderr": 0.044405219061793254,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.04336432707993176
    },
    "mmlu-computer_security": {
      "acc": 0.41,
      "num_samples": 100,
      "acc_stderr": 0.049431107042371025,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "mmlu-conceptual_physics": {
      "acc": 0.31063829787234043,
      "num_samples": 235,
      "acc_stderr": 0.030251237579213167,
      "acc_norm": 0.22127659574468084,
      "acc_norm_stderr": 0.027136349602424063
    },
    "mmlu-econometrics": {
      "acc": 0.30701754385964913,
      "num_samples": 114,
      "acc_stderr": 0.043391383225798615,
      "acc_norm": 0.24561403508771928,
      "acc_norm_stderr": 0.04049339297748141
    },
    "mmlu-electrical_engineering": {
      "acc": 0.33793103448275863,
      "num_samples": 145,
      "acc_stderr": 0.039417076320648906,
      "acc_norm": 0.3586206896551724,
      "acc_norm_stderr": 0.039966295748767186
    },
    "mmlu-elementary_mathematics": {
      "acc": 0.30158730158730157,
      "num_samples": 378,
      "acc_stderr": 0.02363697599610179,
      "acc_norm": 0.30423280423280424,
      "acc_norm_stderr": 0.023695415009463087
    },
    "mmlu-formal_logic": {
      "acc": 0.31746031746031744,
      "num_samples": 126,
      "acc_stderr": 0.04163453031302859,
      "acc_norm": 0.2698412698412698,
      "acc_norm_stderr": 0.039701582732351734
    },
    "mmlu-global_facts": {
      "acc": 0.24,
      "num_samples": 100,
      "acc_stderr": 0.04292346959909284,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.040201512610368445
    },
    "mmlu-high_school_biology": {
      "acc": 0.3387096774193548,
      "num_samples": 310,
      "acc_stderr": 0.02692344605930284,
      "acc_norm": 0.3387096774193548,
      "acc_norm_stderr": 0.026923446059302844
    },
    "mmlu-high_school_chemistry": {
      "acc": 0.27586206896551724,
      "num_samples": 203,
      "acc_stderr": 0.031447125816782426,
      "acc_norm": 0.31527093596059114,
      "acc_norm_stderr": 0.03269080871970186
    },
    "mmlu-high_school_computer_science": {
      "acc": 0.35,
      "num_samples": 100,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252606
    },
    "mmlu-high_school_european_history": {
      "acc": 0.4121212121212121,
      "num_samples": 165,
      "acc_stderr": 0.03843566993588717,
      "acc_norm": 0.3393939393939394,
      "acc_norm_stderr": 0.036974422050315967
    },
    "mmlu-high_school_geography": {
      "acc": 0.30808080808080807,
      "num_samples": 198,
      "acc_stderr": 0.03289477330098616,
      "acc_norm": 0.30808080808080807,
      "acc_norm_stderr": 0.03289477330098617
    },
    "mmlu-high_school_government_and_politics": {
      "acc": 0.39896373056994816,
      "num_samples": 193,
      "acc_stderr": 0.03533999094065695,
      "acc_norm": 0.33678756476683935,
      "acc_norm_stderr": 0.03410780251836184
    },
    "mmlu-high_school_macroeconomics": {
      "acc": 0.32051282051282054,
      "num_samples": 390,
      "acc_stderr": 0.023661296393964273,
      "acc_norm": 0.28974358974358977,
      "acc_norm_stderr": 0.023000628243687954
    },
    "mmlu-high_school_mathematics": {
      "acc": 0.22962962962962963,
      "num_samples": 270,
      "acc_stderr": 0.025644108639267624,
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.026719240783712166
    },
    "mmlu-high_school_microeconomics": {
      "acc": 0.3277310924369748,
      "num_samples": 238,
      "acc_stderr": 0.030489911417673224,
      "acc_norm": 0.3907563025210084,
      "acc_norm_stderr": 0.031693802357129965
    },
    "mmlu-high_school_physics": {
      "acc": 0.31788079470198677,
      "num_samples": 151,
      "acc_stderr": 0.038020397601079024,
      "acc_norm": 0.26490066225165565,
      "acc_norm_stderr": 0.03603038545360384
    },
    "mmlu-high_school_psychology": {
      "acc": 0.3743119266055046,
      "num_samples": 545,
      "acc_stderr": 0.02074895940898832,
      "acc_norm": 0.29174311926605506,
      "acc_norm_stderr": 0.019489300968876532
    },
    "mmlu-high_school_statistics": {
      "acc": 0.2916666666666667,
      "num_samples": 216,
      "acc_stderr": 0.03099866630456053,
      "acc_norm": 0.2824074074074074,
      "acc_norm_stderr": 0.03070137211151092
    },
    "mmlu-high_school_us_history": {
      "acc": 0.37254901960784315,
      "num_samples": 204,
      "acc_stderr": 0.033933885849584046,
      "acc_norm": 0.3088235294117647,
      "acc_norm_stderr": 0.03242661719827218
    },
    "mmlu-high_school_world_history": {
      "acc": 0.4219409282700422,
      "num_samples": 237,
      "acc_stderr": 0.032148146302403695,
      "acc_norm": 0.37130801687763715,
      "acc_norm_stderr": 0.03145068600744859
    },
    "mmlu-human_aging": {
      "acc": 0.3273542600896861,
      "num_samples": 223,
      "acc_stderr": 0.031493846709941306,
      "acc_norm": 0.22869955156950672,
      "acc_norm_stderr": 0.028188240046929193
    },
    "mmlu-human_sexuality": {
      "acc": 0.5038167938931297,
      "num_samples": 131,
      "acc_stderr": 0.043851623256015534,
      "acc_norm": 0.35877862595419846,
      "acc_norm_stderr": 0.04206739313864908
    },
    "mmlu-international_law": {
      "acc": 0.4380165289256198,
      "num_samples": 121,
      "acc_stderr": 0.045291468044357915,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.04545454545454548
    },
    "mmlu-jurisprudence": {
      "acc": 0.3425925925925926,
      "num_samples": 108,
      "acc_stderr": 0.045879047413018105,
      "acc_norm": 0.4537037037037037,
      "acc_norm_stderr": 0.04812917324536823
    },
    "mmlu-logical_fallacies": {
      "acc": 0.3006134969325153,
      "num_samples": 163,
      "acc_stderr": 0.03602511318806771,
      "acc_norm": 0.3312883435582822,
      "acc_norm_stderr": 0.03697983910025588
    },
    "mmlu-machine_learning": {
      "acc": 0.39285714285714285,
      "num_samples": 112,
      "acc_stderr": 0.046355501356099754,
      "acc_norm": 0.25892857142857145,
      "acc_norm_stderr": 0.041577515398656284
    },
    "mmlu-management": {
      "acc": 0.42718446601941745,
      "num_samples": 103,
      "acc_stderr": 0.04897957737781168,
      "acc_norm": 0.3106796116504854,
      "acc_norm_stderr": 0.04582124160161552
    },
    "mmlu-marketing": {
      "acc": 0.5341880341880342,
      "num_samples": 234,
      "acc_stderr": 0.03267942734081228,
      "acc_norm": 0.47863247863247865,
      "acc_norm_stderr": 0.032726164476349545
    },
    "mmlu-medical_genetics": {
      "acc": 0.32,
      "num_samples": 100,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "mmlu-miscellaneous": {
      "acc": 0.45849297573435505,
      "num_samples": 783,
      "acc_stderr": 0.017818248603465564,
      "acc_norm": 0.3563218390804598,
      "acc_norm_stderr": 0.017125853762755893
    },
    "mmlu-moral_disputes": {
      "acc": 0.3265895953757225,
      "num_samples": 346,
      "acc_stderr": 0.025248264774242826,
      "acc_norm": 0.33815028901734107,
      "acc_norm_stderr": 0.025469770149400172
    },
    "mmlu-moral_scenarios": {
      "acc": 0.23910614525139665,
      "num_samples": 895,
      "acc_stderr": 0.014265554192331146,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "mmlu-nutrition": {
      "acc": 0.40522875816993464,
      "num_samples": 306,
      "acc_stderr": 0.028110928492809065,
      "acc_norm": 0.4150326797385621,
      "acc_norm_stderr": 0.0282135041778241
    },
    "mmlu-philosophy": {
      "acc": 0.3311897106109325,
      "num_samples": 311,
      "acc_stderr": 0.026730620728004917,
      "acc_norm": 0.33762057877813506,
      "acc_norm_stderr": 0.026858825879488558
    },
    "mmlu-prehistory": {
      "acc": 0.4012345679012346,
      "num_samples": 324,
      "acc_stderr": 0.027272582849839796,
      "acc_norm": 0.28703703703703703,
      "acc_norm_stderr": 0.025171041915309684
    },
    "mmlu-professional_accounting": {
      "acc": 0.2801418439716312,
      "num_samples": 282,
      "acc_stderr": 0.02678917235114024,
      "acc_norm": 0.2553191489361702,
      "acc_norm_stderr": 0.026011992930901992
    },
    "mmlu-professional_law": {
      "acc": 0.27249022164276404,
      "num_samples": 1534,
      "acc_stderr": 0.011371658294311538,
      "acc_norm": 0.2926988265971317,
      "acc_norm_stderr": 0.011620949195849526
    },
    "mmlu-professional_medicine": {
      "acc": 0.29411764705882354,
      "num_samples": 272,
      "acc_stderr": 0.02767846864214471,
      "acc_norm": 0.28308823529411764,
      "acc_norm_stderr": 0.02736586113151381
    },
    "mmlu-professional_psychology": {
      "acc": 0.34967320261437906,
      "num_samples": 612,
      "acc_stderr": 0.01929196189506637,
      "acc_norm": 0.28104575163398693,
      "acc_norm_stderr": 0.018185218954318082
    },
    "mmlu-public_relations": {
      "acc": 0.38181818181818183,
      "num_samples": 110,
      "acc_stderr": 0.04653429807913508,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.04172343038705382
    },
    "mmlu-security_studies": {
      "acc": 0.4530612244897959,
      "num_samples": 245,
      "acc_stderr": 0.03186785930004129,
      "acc_norm": 0.33877551020408164,
      "acc_norm_stderr": 0.03029950656215418
    },
    "mmlu-sociology": {
      "acc": 0.3781094527363184,
      "num_samples": 201,
      "acc_stderr": 0.03428867848778656,
      "acc_norm": 0.31343283582089554,
      "acc_norm_stderr": 0.03280188205348644
    },
    "mmlu-us_foreign_policy": {
      "acc": 0.5,
      "num_samples": 100,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.049756985195624284
    },
    "mmlu-virology": {
      "acc": 0.3132530120481928,
      "num_samples": 166,
      "acc_stderr": 0.03610805018031024,
      "acc_norm": 0.28313253012048195,
      "acc_norm_stderr": 0.03507295431370518
    },
    "mmlu-world_religions": {
      "acc": 0.49122807017543857,
      "num_samples": 171,
      "acc_stderr": 0.03834234744164993,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.0381107966983353
    }
  },
  "versions": {
    "mmlu-abstract_algebra": 0,
    "mmlu-anatomy": 0,
    "mmlu-astronomy": 0,
    "mmlu-business_ethics": 0,
    "mmlu-clinical_knowledge": 0,
    "mmlu-college_biology": 0,
    "mmlu-college_chemistry": 0,
    "mmlu-college_computer_science": 0,
    "mmlu-college_mathematics": 0,
    "mmlu-college_medicine": 0,
    "mmlu-college_physics": 0,
    "mmlu-computer_security": 0,
    "mmlu-conceptual_physics": 0,
    "mmlu-econometrics": 0,
    "mmlu-electrical_engineering": 0,
    "mmlu-elementary_mathematics": 0,
    "mmlu-formal_logic": 0,
    "mmlu-global_facts": 0,
    "mmlu-high_school_biology": 0,
    "mmlu-high_school_chemistry": 0,
    "mmlu-high_school_computer_science": 0,
    "mmlu-high_school_european_history": 0,
    "mmlu-high_school_geography": 0,
    "mmlu-high_school_government_and_politics": 0,
    "mmlu-high_school_macroeconomics": 0,
    "mmlu-high_school_mathematics": 0,
    "mmlu-high_school_microeconomics": 0,
    "mmlu-high_school_physics": 0,
    "mmlu-high_school_psychology": 0,
    "mmlu-high_school_statistics": 0,
    "mmlu-high_school_us_history": 0,
    "mmlu-high_school_world_history": 0,
    "mmlu-human_aging": 0,
    "mmlu-human_sexuality": 0,
    "mmlu-international_law": 0,
    "mmlu-jurisprudence": 0,
    "mmlu-logical_fallacies": 0,
    "mmlu-machine_learning": 0,
    "mmlu-management": 0,
    "mmlu-marketing": 0,
    "mmlu-medical_genetics": 0,
    "mmlu-miscellaneous": 0,
    "mmlu-moral_disputes": 0,
    "mmlu-moral_scenarios": 0,
    "mmlu-nutrition": 0,
    "mmlu-philosophy": 0,
    "mmlu-prehistory": 0,
    "mmlu-professional_accounting": 0,
    "mmlu-professional_law": 0,
    "mmlu-professional_medicine": 0,
    "mmlu-professional_psychology": 0,
    "mmlu-public_relations": 0,
    "mmlu-security_studies": 0,
    "mmlu-sociology": 0,
    "mmlu-us_foreign_policy": 0,
    "mmlu-virology": 0,
    "mmlu-world_religions": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "use_accelerate=True,pretrained=Barkavi/llama2-7B",
    "num_fewshot": 0,
    "batch_size": null,
    "device": "cuda",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}