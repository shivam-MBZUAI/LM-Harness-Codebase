{
  "results": {
    "mmlu-abstract_algebra": {
      "acc": 0.25,
      "num_samples": 100,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.040201512610368445
    },
    "mmlu-anatomy": {
      "acc": 0.32592592592592595,
      "num_samples": 135,
      "acc_stderr": 0.040491220417025055,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.038201699145179055
    },
    "mmlu-astronomy": {
      "acc": 0.45394736842105265,
      "num_samples": 152,
      "acc_stderr": 0.04051646342874143,
      "acc_norm": 0.4473684210526316,
      "acc_norm_stderr": 0.04046336883978251
    },
    "mmlu-business_ethics": {
      "acc": 0.49,
      "num_samples": 100,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "mmlu-clinical_knowledge": {
      "acc": 0.3471698113207547,
      "num_samples": 265,
      "acc_stderr": 0.02930010170554965,
      "acc_norm": 0.3849056603773585,
      "acc_norm_stderr": 0.02994649856769995
    },
    "mmlu-college_biology": {
      "acc": 0.2916666666666667,
      "num_samples": 144,
      "acc_stderr": 0.03800968060554857,
      "acc_norm": 0.2847222222222222,
      "acc_norm_stderr": 0.03773809990686934
    },
    "mmlu-college_chemistry": {
      "acc": 0.29,
      "num_samples": 100,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "mmlu-college_computer_science": {
      "acc": 0.33,
      "num_samples": 100,
      "acc_stderr": 0.04725815626252605,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.04461960433384741
    },
    "mmlu-college_mathematics": {
      "acc": 0.25,
      "num_samples": 100,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "mmlu-college_medicine": {
      "acc": 0.2774566473988439,
      "num_samples": 173,
      "acc_stderr": 0.034140140070440354,
      "acc_norm": 0.30057803468208094,
      "acc_norm_stderr": 0.0349610148119118
    },
    "mmlu-college_physics": {
      "acc": 0.27450980392156865,
      "num_samples": 102,
      "acc_stderr": 0.04440521906179324,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.04336432707993176
    },
    "mmlu-computer_security": {
      "acc": 0.41,
      "num_samples": 100,
      "acc_stderr": 0.04943110704237101,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.05
    },
    "mmlu-conceptual_physics": {
      "acc": 0.3021276595744681,
      "num_samples": 235,
      "acc_stderr": 0.030017554471880554,
      "acc_norm": 0.20851063829787234,
      "acc_norm_stderr": 0.026556982117838728
    },
    "mmlu-econometrics": {
      "acc": 0.35964912280701755,
      "num_samples": 114,
      "acc_stderr": 0.04514496132873633,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.044346007015849245
    },
    "mmlu-electrical_engineering": {
      "acc": 0.32413793103448274,
      "num_samples": 145,
      "acc_stderr": 0.03900432069185553,
      "acc_norm": 0.31724137931034485,
      "acc_norm_stderr": 0.038783523721386215
    },
    "mmlu-elementary_mathematics": {
      "acc": 0.32275132275132273,
      "num_samples": 378,
      "acc_stderr": 0.024078943243597016,
      "acc_norm": 0.3253968253968254,
      "acc_norm_stderr": 0.024130158299762613
    },
    "mmlu-formal_logic": {
      "acc": 0.24603174603174602,
      "num_samples": 126,
      "acc_stderr": 0.038522733649243156,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.04006168083848877
    },
    "mmlu-global_facts": {
      "acc": 0.25,
      "num_samples": 100,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816506
    },
    "mmlu-high_school_biology": {
      "acc": 0.4064516129032258,
      "num_samples": 310,
      "acc_stderr": 0.027941727346256315,
      "acc_norm": 0.3419354838709677,
      "acc_norm_stderr": 0.026985289576552725
    },
    "mmlu-high_school_chemistry": {
      "acc": 0.2660098522167488,
      "num_samples": 203,
      "acc_stderr": 0.03108982600293752,
      "acc_norm": 0.3103448275862069,
      "acc_norm_stderr": 0.032550867699701024
    },
    "mmlu-high_school_computer_science": {
      "acc": 0.37,
      "num_samples": 100,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "mmlu-high_school_european_history": {
      "acc": 0.4727272727272727,
      "num_samples": 165,
      "acc_stderr": 0.0389853160557942,
      "acc_norm": 0.42424242424242425,
      "acc_norm_stderr": 0.03859268142070262
    },
    "mmlu-high_school_geography": {
      "acc": 0.40404040404040403,
      "num_samples": 198,
      "acc_stderr": 0.03496130972056127,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.034273086529999344
    },
    "mmlu-high_school_government_and_politics": {
      "acc": 0.45595854922279794,
      "num_samples": 193,
      "acc_stderr": 0.03594413711272437,
      "acc_norm": 0.35751295336787564,
      "acc_norm_stderr": 0.03458816042181005
    },
    "mmlu-high_school_macroeconomics": {
      "acc": 0.35384615384615387,
      "num_samples": 390,
      "acc_stderr": 0.024243783994062164,
      "acc_norm": 0.30256410256410254,
      "acc_norm_stderr": 0.023290888053772725
    },
    "mmlu-high_school_mathematics": {
      "acc": 0.22962962962962963,
      "num_samples": 270,
      "acc_stderr": 0.025644108639267624,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.0279404571362284
    },
    "mmlu-high_school_microeconomics": {
      "acc": 0.4369747899159664,
      "num_samples": 238,
      "acc_stderr": 0.032219436365661956,
      "acc_norm": 0.3949579831932773,
      "acc_norm_stderr": 0.03175367846096626
    },
    "mmlu-high_school_physics": {
      "acc": 0.24503311258278146,
      "num_samples": 151,
      "acc_stderr": 0.035118075718047224,
      "acc_norm": 0.2847682119205298,
      "acc_norm_stderr": 0.03684881521389023
    },
    "mmlu-high_school_psychology": {
      "acc": 0.3743119266055046,
      "num_samples": 545,
      "acc_stderr": 0.020748959408988313,
      "acc_norm": 0.29357798165137616,
      "acc_norm_stderr": 0.019525151122639667
    },
    "mmlu-high_school_statistics": {
      "acc": 0.37962962962962965,
      "num_samples": 216,
      "acc_stderr": 0.03309682581119035,
      "acc_norm": 0.3611111111111111,
      "acc_norm_stderr": 0.03275773486100999
    },
    "mmlu-high_school_us_history": {
      "acc": 0.45098039215686275,
      "num_samples": 204,
      "acc_stderr": 0.03492406104163613,
      "acc_norm": 0.37254901960784315,
      "acc_norm_stderr": 0.03393388584958404
    },
    "mmlu-high_school_world_history": {
      "acc": 0.4978902953586498,
      "num_samples": 237,
      "acc_stderr": 0.032546938018020076,
      "acc_norm": 0.4345991561181435,
      "acc_norm_stderr": 0.03226759995510145
    },
    "mmlu-human_aging": {
      "acc": 0.2914798206278027,
      "num_samples": 223,
      "acc_stderr": 0.0305002831765459,
      "acc_norm": 0.22869955156950672,
      "acc_norm_stderr": 0.028188240046929193
    },
    "mmlu-human_sexuality": {
      "acc": 0.5114503816793893,
      "num_samples": 131,
      "acc_stderr": 0.043841400240780176,
      "acc_norm": 0.37404580152671757,
      "acc_norm_stderr": 0.04243869242230524
    },
    "mmlu-international_law": {
      "acc": 0.49586776859504134,
      "num_samples": 121,
      "acc_stderr": 0.04564198767432754,
      "acc_norm": 0.6033057851239669,
      "acc_norm_stderr": 0.04465869780531009
    },
    "mmlu-jurisprudence": {
      "acc": 0.4166666666666667,
      "num_samples": 108,
      "acc_stderr": 0.04766075165356461,
      "acc_norm": 0.4537037037037037,
      "acc_norm_stderr": 0.04812917324536823
    },
    "mmlu-logical_fallacies": {
      "acc": 0.3803680981595092,
      "num_samples": 163,
      "acc_stderr": 0.038142698932618374,
      "acc_norm": 0.37423312883435583,
      "acc_norm_stderr": 0.03802068102899615
    },
    "mmlu-machine_learning": {
      "acc": 0.29464285714285715,
      "num_samples": 112,
      "acc_stderr": 0.04327040932578728,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.042878587513404544
    },
    "mmlu-management": {
      "acc": 0.47572815533980584,
      "num_samples": 103,
      "acc_stderr": 0.049449010929737795,
      "acc_norm": 0.42718446601941745,
      "acc_norm_stderr": 0.048979577377811674
    },
    "mmlu-marketing": {
      "acc": 0.3974358974358974,
      "num_samples": 234,
      "acc_stderr": 0.03205953453789293,
      "acc_norm": 0.44017094017094016,
      "acc_norm_stderr": 0.032520741720630506
    },
    "mmlu-medical_genetics": {
      "acc": 0.35,
      "num_samples": 100,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "mmlu-miscellaneous": {
      "acc": 0.4891443167305236,
      "num_samples": 783,
      "acc_stderr": 0.017875748840242418,
      "acc_norm": 0.35887611749680715,
      "acc_norm_stderr": 0.017152991797501342
    },
    "mmlu-moral_disputes": {
      "acc": 0.3208092485549133,
      "num_samples": 346,
      "acc_stderr": 0.025131000233647907,
      "acc_norm": 0.3179190751445087,
      "acc_norm_stderr": 0.025070713719153176
    },
    "mmlu-moral_scenarios": {
      "acc": 0.28044692737430166,
      "num_samples": 895,
      "acc_stderr": 0.01502408388332291,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "mmlu-nutrition": {
      "acc": 0.4215686274509804,
      "num_samples": 306,
      "acc_stderr": 0.02827549015679143,
      "acc_norm": 0.45751633986928103,
      "acc_norm_stderr": 0.02852638345214263
    },
    "mmlu-philosophy": {
      "acc": 0.3987138263665595,
      "num_samples": 311,
      "acc_stderr": 0.0278093225857745,
      "acc_norm": 0.3440514469453376,
      "acc_norm_stderr": 0.02698147804364803
    },
    "mmlu-prehistory": {
      "acc": 0.36419753086419754,
      "num_samples": 324,
      "acc_stderr": 0.02677492989972233,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.02563082497562134
    },
    "mmlu-professional_accounting": {
      "acc": 0.25177304964539005,
      "num_samples": 282,
      "acc_stderr": 0.0258921511567094,
      "acc_norm": 0.2198581560283688,
      "acc_norm_stderr": 0.024706141070705474
    },
    "mmlu-professional_law": {
      "acc": 0.33116036505867014,
      "num_samples": 1534,
      "acc_stderr": 0.012020128195985746,
      "acc_norm": 0.3324641460234681,
      "acc_norm_stderr": 0.012032022332260516
    },
    "mmlu-professional_medicine": {
      "acc": 0.30514705882352944,
      "num_samples": 272,
      "acc_stderr": 0.027971541370170595,
      "acc_norm": 0.28308823529411764,
      "acc_norm_stderr": 0.02736586113151381
    },
    "mmlu-professional_psychology": {
      "acc": 0.30718954248366015,
      "num_samples": 612,
      "acc_stderr": 0.018663359671463663,
      "acc_norm": 0.272875816993464,
      "acc_norm_stderr": 0.018020474148393577
    },
    "mmlu-public_relations": {
      "acc": 0.4909090909090909,
      "num_samples": 110,
      "acc_stderr": 0.04788339768702861,
      "acc_norm": 0.19090909090909092,
      "acc_norm_stderr": 0.03764425585984925
    },
    "mmlu-security_studies": {
      "acc": 0.5020408163265306,
      "num_samples": 245,
      "acc_stderr": 0.0320089533497105,
      "acc_norm": 0.3877551020408163,
      "acc_norm_stderr": 0.031192230726795656
    },
    "mmlu-sociology": {
      "acc": 0.34328358208955223,
      "num_samples": 201,
      "acc_stderr": 0.03357379665433431,
      "acc_norm": 0.3034825870646766,
      "acc_norm_stderr": 0.032510068164586174
    },
    "mmlu-us_foreign_policy": {
      "acc": 0.5,
      "num_samples": 100,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.049756985195624284
    },
    "mmlu-virology": {
      "acc": 0.39759036144578314,
      "num_samples": 166,
      "acc_stderr": 0.038099730845402184,
      "acc_norm": 0.3674698795180723,
      "acc_norm_stderr": 0.03753267402120575
    },
    "mmlu-world_religions": {
      "acc": 0.47368421052631576,
      "num_samples": 171,
      "acc_stderr": 0.03829509868994727,
      "acc_norm": 0.47953216374269003,
      "acc_norm_stderr": 0.038316105328219316
    }
  },
  "versions": {
    "mmlu-abstract_algebra": 0,
    "mmlu-anatomy": 0,
    "mmlu-astronomy": 0,
    "mmlu-business_ethics": 0,
    "mmlu-clinical_knowledge": 0,
    "mmlu-college_biology": 0,
    "mmlu-college_chemistry": 0,
    "mmlu-college_computer_science": 0,
    "mmlu-college_mathematics": 0,
    "mmlu-college_medicine": 0,
    "mmlu-college_physics": 0,
    "mmlu-computer_security": 0,
    "mmlu-conceptual_physics": 0,
    "mmlu-econometrics": 0,
    "mmlu-electrical_engineering": 0,
    "mmlu-elementary_mathematics": 0,
    "mmlu-formal_logic": 0,
    "mmlu-global_facts": 0,
    "mmlu-high_school_biology": 0,
    "mmlu-high_school_chemistry": 0,
    "mmlu-high_school_computer_science": 0,
    "mmlu-high_school_european_history": 0,
    "mmlu-high_school_geography": 0,
    "mmlu-high_school_government_and_politics": 0,
    "mmlu-high_school_macroeconomics": 0,
    "mmlu-high_school_mathematics": 0,
    "mmlu-high_school_microeconomics": 0,
    "mmlu-high_school_physics": 0,
    "mmlu-high_school_psychology": 0,
    "mmlu-high_school_statistics": 0,
    "mmlu-high_school_us_history": 0,
    "mmlu-high_school_world_history": 0,
    "mmlu-human_aging": 0,
    "mmlu-human_sexuality": 0,
    "mmlu-international_law": 0,
    "mmlu-jurisprudence": 0,
    "mmlu-logical_fallacies": 0,
    "mmlu-machine_learning": 0,
    "mmlu-management": 0,
    "mmlu-marketing": 0,
    "mmlu-medical_genetics": 0,
    "mmlu-miscellaneous": 0,
    "mmlu-moral_disputes": 0,
    "mmlu-moral_scenarios": 0,
    "mmlu-nutrition": 0,
    "mmlu-philosophy": 0,
    "mmlu-prehistory": 0,
    "mmlu-professional_accounting": 0,
    "mmlu-professional_law": 0,
    "mmlu-professional_medicine": 0,
    "mmlu-professional_psychology": 0,
    "mmlu-public_relations": 0,
    "mmlu-security_studies": 0,
    "mmlu-sociology": 0,
    "mmlu-us_foreign_policy": 0,
    "mmlu-virology": 0,
    "mmlu-world_religions": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "use_accelerate=True,pretrained=CohereForAI/aya-23-8B",
    "num_fewshot": 0,
    "batch_size": null,
    "device": "cuda",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}