{
  "results": {
    "mmlu-abstract_algebra": {
      "acc": 0.26,
      "num_samples": 100,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "mmlu-anatomy": {
      "acc": 0.34814814814814815,
      "num_samples": 135,
      "acc_stderr": 0.041153246103369526,
      "acc_norm": 0.2518518518518518,
      "acc_norm_stderr": 0.03749850709174022
    },
    "mmlu-astronomy": {
      "acc": 0.506578947368421,
      "num_samples": 152,
      "acc_stderr": 0.040685900502249704,
      "acc_norm": 0.5197368421052632,
      "acc_norm_stderr": 0.040657710025626036
    },
    "mmlu-business_ethics": {
      "acc": 0.59,
      "num_samples": 100,
      "acc_stderr": 0.049431107042371025,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.050211673156867795
    },
    "mmlu-clinical_knowledge": {
      "acc": 0.4,
      "num_samples": 265,
      "acc_stderr": 0.030151134457776296,
      "acc_norm": 0.4226415094339623,
      "acc_norm_stderr": 0.030402331445769537
    },
    "mmlu-college_biology": {
      "acc": 0.3819444444444444,
      "num_samples": 144,
      "acc_stderr": 0.040629907841466674,
      "acc_norm": 0.3194444444444444,
      "acc_norm_stderr": 0.038990736873573344
    },
    "mmlu-college_chemistry": {
      "acc": 0.31,
      "num_samples": 100,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.04461960433384741
    },
    "mmlu-college_computer_science": {
      "acc": 0.39,
      "num_samples": 100,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "mmlu-college_mathematics": {
      "acc": 0.3,
      "num_samples": 100,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "mmlu-college_medicine": {
      "acc": 0.37572254335260113,
      "num_samples": 173,
      "acc_stderr": 0.03692820767264867,
      "acc_norm": 0.31213872832369943,
      "acc_norm_stderr": 0.03533133389323657
    },
    "mmlu-college_physics": {
      "acc": 0.28431372549019607,
      "num_samples": 102,
      "acc_stderr": 0.04488482852329017,
      "acc_norm": 0.3235294117647059,
      "acc_norm_stderr": 0.046550104113196177
    },
    "mmlu-computer_security": {
      "acc": 0.5,
      "num_samples": 100,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.04988876515698589
    },
    "mmlu-conceptual_physics": {
      "acc": 0.33191489361702126,
      "num_samples": 235,
      "acc_stderr": 0.030783736757745657,
      "acc_norm": 0.2425531914893617,
      "acc_norm_stderr": 0.02802022627120022
    },
    "mmlu-econometrics": {
      "acc": 0.30701754385964913,
      "num_samples": 114,
      "acc_stderr": 0.043391383225798615,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.041424397194893624
    },
    "mmlu-electrical_engineering": {
      "acc": 0.38620689655172413,
      "num_samples": 145,
      "acc_stderr": 0.04057324734419035,
      "acc_norm": 0.3931034482758621,
      "acc_norm_stderr": 0.040703290137070705
    },
    "mmlu-elementary_mathematics": {
      "acc": 0.3412698412698413,
      "num_samples": 378,
      "acc_stderr": 0.024419234966819074,
      "acc_norm": 0.3386243386243386,
      "acc_norm_stderr": 0.024373197867983056
    },
    "mmlu-formal_logic": {
      "acc": 0.31746031746031744,
      "num_samples": 126,
      "acc_stderr": 0.041634530313028585,
      "acc_norm": 0.29365079365079366,
      "acc_norm_stderr": 0.04073524322147126
    },
    "mmlu-global_facts": {
      "acc": 0.35,
      "num_samples": 100,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "mmlu-high_school_biology": {
      "acc": 0.5,
      "num_samples": 310,
      "acc_stderr": 0.028444006199428714,
      "acc_norm": 0.3774193548387097,
      "acc_norm_stderr": 0.02757596072327823
    },
    "mmlu-high_school_chemistry": {
      "acc": 0.3251231527093596,
      "num_samples": 203,
      "acc_stderr": 0.03295797566311271,
      "acc_norm": 0.35467980295566504,
      "acc_norm_stderr": 0.033661244890514495
    },
    "mmlu-high_school_computer_science": {
      "acc": 0.39,
      "num_samples": 100,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "mmlu-high_school_european_history": {
      "acc": 0.5515151515151515,
      "num_samples": 165,
      "acc_stderr": 0.038835659779569286,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.038881769216741004
    },
    "mmlu-high_school_geography": {
      "acc": 0.5252525252525253,
      "num_samples": 198,
      "acc_stderr": 0.035578062450873145,
      "acc_norm": 0.35353535353535354,
      "acc_norm_stderr": 0.03406086723547153
    },
    "mmlu-high_school_government_and_politics": {
      "acc": 0.5803108808290155,
      "num_samples": 193,
      "acc_stderr": 0.035615873276858834,
      "acc_norm": 0.42487046632124353,
      "acc_norm_stderr": 0.0356747133521254
    },
    "mmlu-high_school_macroeconomics": {
      "acc": 0.4512820512820513,
      "num_samples": 390,
      "acc_stderr": 0.025230381238934833,
      "acc_norm": 0.382051282051282,
      "acc_norm_stderr": 0.02463554916390823
    },
    "mmlu-high_school_mathematics": {
      "acc": 0.34444444444444444,
      "num_samples": 270,
      "acc_stderr": 0.028972648884844267,
      "acc_norm": 0.37407407407407406,
      "acc_norm_stderr": 0.029502861128955286
    },
    "mmlu-high_school_microeconomics": {
      "acc": 0.44537815126050423,
      "num_samples": 238,
      "acc_stderr": 0.0322841062671639,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.03214536859788639
    },
    "mmlu-high_school_physics": {
      "acc": 0.33774834437086093,
      "num_samples": 151,
      "acc_stderr": 0.038615575462551684,
      "acc_norm": 0.31788079470198677,
      "acc_norm_stderr": 0.038020397601079024
    },
    "mmlu-high_school_psychology": {
      "acc": 0.46422018348623856,
      "num_samples": 545,
      "acc_stderr": 0.0213823647757019,
      "acc_norm": 0.3100917431192661,
      "acc_norm_stderr": 0.01983084968443975
    },
    "mmlu-high_school_statistics": {
      "acc": 0.36574074074074076,
      "num_samples": 216,
      "acc_stderr": 0.032847388576472056,
      "acc_norm": 0.35648148148148145,
      "acc_norm_stderr": 0.032664783315272714
    },
    "mmlu-high_school_us_history": {
      "acc": 0.5245098039215687,
      "num_samples": 204,
      "acc_stderr": 0.03505093194348798,
      "acc_norm": 0.4362745098039216,
      "acc_norm_stderr": 0.03480693138457039
    },
    "mmlu-high_school_world_history": {
      "acc": 0.5864978902953587,
      "num_samples": 237,
      "acc_stderr": 0.03205649904851858,
      "acc_norm": 0.4767932489451477,
      "acc_norm_stderr": 0.032512152011410174
    },
    "mmlu-human_aging": {
      "acc": 0.4125560538116592,
      "num_samples": 223,
      "acc_stderr": 0.03304062175449297,
      "acc_norm": 0.23766816143497757,
      "acc_norm_stderr": 0.028568079464714274
    },
    "mmlu-human_sexuality": {
      "acc": 0.4961832061068702,
      "num_samples": 131,
      "acc_stderr": 0.043851623256015534,
      "acc_norm": 0.29770992366412213,
      "acc_norm_stderr": 0.040103589424622034
    },
    "mmlu-international_law": {
      "acc": 0.5289256198347108,
      "num_samples": 121,
      "acc_stderr": 0.04556710331269498,
      "acc_norm": 0.6694214876033058,
      "acc_norm_stderr": 0.04294340845212094
    },
    "mmlu-jurisprudence": {
      "acc": 0.5648148148148148,
      "num_samples": 108,
      "acc_stderr": 0.04792898170907062,
      "acc_norm": 0.49074074074074076,
      "acc_norm_stderr": 0.04832853553437055
    },
    "mmlu-logical_fallacies": {
      "acc": 0.43558282208588955,
      "num_samples": 163,
      "acc_stderr": 0.038956324641389366,
      "acc_norm": 0.3619631901840491,
      "acc_norm_stderr": 0.037757007291414416
    },
    "mmlu-machine_learning": {
      "acc": 0.30357142857142855,
      "num_samples": 112,
      "acc_stderr": 0.04364226155841044,
      "acc_norm": 0.29464285714285715,
      "acc_norm_stderr": 0.0432704093257873
    },
    "mmlu-management": {
      "acc": 0.4854368932038835,
      "num_samples": 103,
      "acc_stderr": 0.049486373240266376,
      "acc_norm": 0.4174757281553398,
      "acc_norm_stderr": 0.048828405482122375
    },
    "mmlu-marketing": {
      "acc": 0.688034188034188,
      "num_samples": 234,
      "acc_stderr": 0.030351527323344948,
      "acc_norm": 0.5384615384615384,
      "acc_norm_stderr": 0.03265903381186195
    },
    "mmlu-medical_genetics": {
      "acc": 0.37,
      "num_samples": 100,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.049756985195624284
    },
    "mmlu-miscellaneous": {
      "acc": 0.5402298850574713,
      "num_samples": 783,
      "acc_stderr": 0.01782199409693354,
      "acc_norm": 0.3550446998722861,
      "acc_norm_stderr": 0.017112085772773
    },
    "mmlu-moral_disputes": {
      "acc": 0.3988439306358382,
      "num_samples": 346,
      "acc_stderr": 0.026362437574546545,
      "acc_norm": 0.3699421965317919,
      "acc_norm_stderr": 0.02599247202930638
    },
    "mmlu-moral_scenarios": {
      "acc": 0.24692737430167597,
      "num_samples": 895,
      "acc_stderr": 0.014422292204808835,
      "acc_norm": 0.2860335195530726,
      "acc_norm_stderr": 0.015113972129062151
    },
    "mmlu-nutrition": {
      "acc": 0.434640522875817,
      "num_samples": 306,
      "acc_stderr": 0.028384256704883034,
      "acc_norm": 0.4117647058823529,
      "acc_norm_stderr": 0.02818059632825929
    },
    "mmlu-philosophy": {
      "acc": 0.4115755627009646,
      "num_samples": 311,
      "acc_stderr": 0.027950481494401262,
      "acc_norm": 0.3247588424437299,
      "acc_norm_stderr": 0.026596782287697046
    },
    "mmlu-prehistory": {
      "acc": 0.4506172839506173,
      "num_samples": 324,
      "acc_stderr": 0.027684721415656203,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.02584224870090217
    },
    "mmlu-professional_accounting": {
      "acc": 0.3049645390070922,
      "num_samples": 282,
      "acc_stderr": 0.027464708442022128,
      "acc_norm": 0.30141843971631205,
      "acc_norm_stderr": 0.02737412888263115
    },
    "mmlu-professional_law": {
      "acc": 0.35984354628422427,
      "num_samples": 1534,
      "acc_stderr": 0.012258260483689797,
      "acc_norm": 0.34876140808344197,
      "acc_norm_stderr": 0.012172035157127115
    },
    "mmlu-professional_medicine": {
      "acc": 0.4485294117647059,
      "num_samples": 272,
      "acc_stderr": 0.030211479609121603,
      "acc_norm": 0.34191176470588236,
      "acc_norm_stderr": 0.028814722422254177
    },
    "mmlu-professional_psychology": {
      "acc": 0.39705882352941174,
      "num_samples": 612,
      "acc_stderr": 0.01979448890002411,
      "acc_norm": 0.29901960784313725,
      "acc_norm_stderr": 0.018521756215423024
    },
    "mmlu-public_relations": {
      "acc": 0.4727272727272727,
      "num_samples": 110,
      "acc_stderr": 0.04782001791380063,
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.040693063197213775
    },
    "mmlu-security_studies": {
      "acc": 0.4897959183673469,
      "num_samples": 245,
      "acc_stderr": 0.032002553478937816,
      "acc_norm": 0.3510204081632653,
      "acc_norm_stderr": 0.03055531675557364
    },
    "mmlu-sociology": {
      "acc": 0.43283582089552236,
      "num_samples": 201,
      "acc_stderr": 0.03503490923673281,
      "acc_norm": 0.373134328358209,
      "acc_norm_stderr": 0.034198326081760065
    },
    "mmlu-us_foreign_policy": {
      "acc": 0.64,
      "num_samples": 100,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.05016135580465919
    },
    "mmlu-virology": {
      "acc": 0.42771084337349397,
      "num_samples": 166,
      "acc_stderr": 0.03851597683718534,
      "acc_norm": 0.3493975903614458,
      "acc_norm_stderr": 0.037117251907407486
    },
    "mmlu-world_religions": {
      "acc": 0.6023391812865497,
      "num_samples": 171,
      "acc_stderr": 0.0375363895576169,
      "acc_norm": 0.5380116959064327,
      "acc_norm_stderr": 0.03823727092882307
    }
  },
  "versions": {
    "mmlu-abstract_algebra": 0,
    "mmlu-anatomy": 0,
    "mmlu-astronomy": 0,
    "mmlu-business_ethics": 0,
    "mmlu-clinical_knowledge": 0,
    "mmlu-college_biology": 0,
    "mmlu-college_chemistry": 0,
    "mmlu-college_computer_science": 0,
    "mmlu-college_mathematics": 0,
    "mmlu-college_medicine": 0,
    "mmlu-college_physics": 0,
    "mmlu-computer_security": 0,
    "mmlu-conceptual_physics": 0,
    "mmlu-econometrics": 0,
    "mmlu-electrical_engineering": 0,
    "mmlu-elementary_mathematics": 0,
    "mmlu-formal_logic": 0,
    "mmlu-global_facts": 0,
    "mmlu-high_school_biology": 0,
    "mmlu-high_school_chemistry": 0,
    "mmlu-high_school_computer_science": 0,
    "mmlu-high_school_european_history": 0,
    "mmlu-high_school_geography": 0,
    "mmlu-high_school_government_and_politics": 0,
    "mmlu-high_school_macroeconomics": 0,
    "mmlu-high_school_mathematics": 0,
    "mmlu-high_school_microeconomics": 0,
    "mmlu-high_school_physics": 0,
    "mmlu-high_school_psychology": 0,
    "mmlu-high_school_statistics": 0,
    "mmlu-high_school_us_history": 0,
    "mmlu-high_school_world_history": 0,
    "mmlu-human_aging": 0,
    "mmlu-human_sexuality": 0,
    "mmlu-international_law": 0,
    "mmlu-jurisprudence": 0,
    "mmlu-logical_fallacies": 0,
    "mmlu-machine_learning": 0,
    "mmlu-management": 0,
    "mmlu-marketing": 0,
    "mmlu-medical_genetics": 0,
    "mmlu-miscellaneous": 0,
    "mmlu-moral_disputes": 0,
    "mmlu-moral_scenarios": 0,
    "mmlu-nutrition": 0,
    "mmlu-philosophy": 0,
    "mmlu-prehistory": 0,
    "mmlu-professional_accounting": 0,
    "mmlu-professional_law": 0,
    "mmlu-professional_medicine": 0,
    "mmlu-professional_psychology": 0,
    "mmlu-public_relations": 0,
    "mmlu-security_studies": 0,
    "mmlu-sociology": 0,
    "mmlu-us_foreign_policy": 0,
    "mmlu-virology": 0,
    "mmlu-world_religions": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "use_accelerate=True,pretrained=sundar-pichai/llama-2-13b",
    "num_fewshot": 0,
    "batch_size": null,
    "device": "cuda",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}