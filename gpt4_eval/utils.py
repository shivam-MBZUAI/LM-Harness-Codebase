import openai
import time
import json

import requests
import sys
import re
import os
import platform
import time, requests

kernel_version = platform.uname().release

# TODO put all the constants at one central location for both lm and gpt4 evals

# if kernel_version == "5.4.0-104-generic":
#     results_path = "/nfs/users/ext_sunil.kumar/nlp_data/LM_DATA/EvaluationResults/gpt4_eval_results"
# elif kernel_version == "5.15.0-1047-oracle":
#     results_path = '/nfs/nlp_data/eval_results/gpt4_eval_results'
# else:
#     raise NotImplementedError

# results_path = "/nfs/users/ext_sunil.kumar/nlp_data/LM_DATA/EvaluationResults/gpt4_eval_results"
# results_path = "/nfs_shared/NLP_DATA/LM_DATA/EvaluationResults/gpt4_eval_results"

results_path = "/vast/core42-nlp/shared/NLP_DATA/LM_DATA/EvaluationResults/gpt4_eval_results"

response_out_path = os.path.join(results_path, "response")
jsonl_out_path = os.path.join(results_path, "comparison/")
xls_out_path = os.path.join(results_path, "comparison/xlsheets/")
gpt4_comp_path = os.path.join(results_path, "fig")

os.makedirs(response_out_path, exist_ok=True)
os.makedirs(jsonl_out_path, exist_ok=True)
os.makedirs(xls_out_path, exist_ok=True)
os.makedirs(gpt4_comp_path, exist_ok=True)

gpt4_eval_path = os.path.dirname(os.path.abspath(__file__))
gen_prompts_file = os.path.join(gpt4_eval_path, "gen_prompts.json")
gpt4_prompts_file = os.path.join(gpt4_eval_path, "gpt4_prompts.json")

# datasets_path = os.path.join(os.path.abspath(os.path.join(gpt4_eval_path, os.pardir)), "datasets")
## Updated the path to local datasets inside gpt4_eval
datasets_path = os.path.abspath(os.path.join(gpt4_eval_path, "datasets"))
vicuna_80_ques = os.path.join(datasets_path, 'vicuna_questions.jsonl')
self_instruct_seeds = os.path.join(datasets_path, 'seed_tasks_questions.jsonl')
summarization_en = os.path.join(datasets_path, 'test_en_summ_generation.jsonl')
summarization_ar = os.path.join(datasets_path, 'test_ar_summ_generation.jsonl')
safety_gen = os.path.join(datasets_path, 'safety_gen.jsonl')
iqeval_path = os.path.join(datasets_path, 'iqeval_data.jsonl')
itc_path = os.path.join(datasets_path, 'test_itc.jsonl')
freshqa_path = os.path.join(datasets_path, 'freshqa_v11132023.jsonl')
quant_path = os.path.join(datasets_path, 'test_quant_itc.jsonl')

MAX_API_RETRY = 10
REQ_TIME_GAP = 1
openai.api_key = "<set your key>"

VICUNA = 'vicuna'  # vicuna 80 questions
SELF_INSTRUCT = 'seeds'  # self-instruct seeds 175 questions
SUMMARIZATION = 'summary'  # summarization dataset 200 articles
SAFETY = 'safety'  # safety dataset 100 prompts
IQEVAL = 'iqeval'
FRESHQA = 'freshqa'
ITC = 'itc'
QUANT = 'quant'

GPT4_PROMPT_TYPE = ["generic", "safety", "summary", "preference", "helpful", "generic_cross", 'single-with-ref', "freshqa"]

TASKS = [VICUNA, SELF_INSTRUCT, SUMMARIZATION, SAFETY, FRESHQA, IQEVAL, ITC, QUANT]
AR = 'ar'
EN = 'en'


def read_json(file_path):
    with open(file_path) as f:
        data = json.load(f)
    return data


def read_jsonl(file_path):
    with open(file_path) as f:
        data = []
        for line in f.readlines():
            data.append(json.loads(line))
    return data


def write_jsonl(data, f_name):
    with open(f_name, 'w') as jsonlfile:
        for i, line in enumerate(data):
            try:
                json.dump(line, jsonlfile, default=str)
                jsonlfile.write('\n')
            except Exception as e:
                print(f"Problem writing at {i} line: line {line}")
                print(e)
                break


def gpt4_gen(sys_prompt, user_prompt: str, max_tokens: int):
    for i in range(MAX_API_RETRY):
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4-0613",
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {
                        "role": "user",
                        "content": user_prompt,
                    },
                ],
                temperature=0.2,
                max_tokens=max_tokens,
            )
            content = response["choices"][0]["message"]["content"]
            return content
        except Exception as e:
            time.sleep(10)
    print(f"Failed after {MAX_API_RETRY} retries.")
    print(f"INPUT: {user_prompt}")
    return "error"

def get_response_out_file_path(model_name, task, lang):
    """Returns file path to model generations `*.jsonl` generated by model_text_gen.py"""
    os.makedirs(response_out_path, exist_ok=True)
    output_file_path = os.path.join(response_out_path, f"{model_name}_{task}_{lang}.jsonl")
    return output_file_path


def get_comparison_out_file_path(model1, model2, gpt4_prompt_type, task, lang):
    comparison_output_filename = f'{model1}_vs_{model2}_{gpt4_prompt_type}_{task}_{lang}'
    comparison_out_jsonl_path = os.path.join(jsonl_out_path, f"{comparison_output_filename}.jsonl")
    comparison_out_xls_path = os.path.join(xls_out_path, f"{comparison_output_filename}.xlsx")
    return comparison_out_jsonl_path, comparison_out_xls_path


def get_single_out_file_path(model, gpt4_prompt_type, task, lang):
    """Returns file path to model scores by "GPT-as-Judge".
    NOTE: Alternative to `get_comparison_out_file_path()` for single model GPT evals.
    """
    single_output_filename = f'{model}_{gpt4_prompt_type}_{task}_{lang}'
    single_out_jsonl_path = os.path.join(jsonl_out_path, f"{single_output_filename}.jsonl")
    single_out_xls_path = os.path.join(xls_out_path, f"{single_output_filename}.xlsx")
    return single_out_jsonl_path, single_out_xls_path


port_dict = {
    'ar2en': 5000,
    'en2ar': 5001,
    'ar2fr': 5002,
    'ar2ru': 5003,
    'de2en': 5004,
    'en2de': 5005,
    'en2es': 5006,
    'en2fr': 5007,
    'en2it': 5008,
    'en2pt': 5009,
    'en2ru': 5010,
    'en2zh': 5011,
    'es2en': 5012,
    'fr2ar': 5013,
    'fr2en': 5014,
    'it2en': 5015,
    'pt2en': 5016,
    'ru2ar': 5017,
    'ru2en': 5018,
    'zh2en': 5019,
    'ar2de': 5022,
    'de2ar': 5023,
    'ar2es': 5024,
    'es2ar': 5025,
    'ar2it': 5026,
    'it2ar': 5027,
    'ar2pt': 5028,
    'pt2ar': 5029,
    'en2fa': 5020
}


def translate(text, model):
    port = port_dict[model]
    url = f'http://188.116.30.85:{port}/translator/translate'
    # url=f'http://10.111.137.61:8022/translator/translate'
    src = text
    payload = [{"id": model, "src": src}]
    payload = json.dumps(payload)
    headers = {'Content-Type': 'application/json'}
    res = requests.post(url, data=payload, headers=headers)
    try:
        res = json.loads(res.text)
        return '\n'.join([d['tgt'] for d in res])
    except:
        print(res)


def parse_score_double(string):
    # string = string.replace(' ','')
    # string = string.replace('\n','')
    n1, n2 = -1, -1  ## -1 means unable to extract scores
    # pattern = r"<score1>([0-9]|10)</score1>"
    # pattern = r"<score1>\s*([0-9]|10)\s*</score1>" ## + whitespaces and newlines
    # pattern = r"<score1>\s*(\d+(\.\d+)?|10)\s*</score1>" ## + decimal numbers
    pattern = r"<score1>\D*(\d+(\.\d+)?|10)\D*</score1>"  ## Ignores non-digit inside tags

    match = re.search(pattern, string)
    if match:
        n1 = int(match.group(1))

    # pattern = r"<score2>([0-9]|10)</score2>"
    # pattern = r"<score2>\s*([0-9]|10)\s*</score2>" ## + whitespaces and newlines
    # pattern = r"<score2>\s*(\d+(\.\d+)?|10)\s*</score2>" ## + decimal numbers
    pattern = r"<score2>\D*(\d+(\.\d+)?|10)\D*</score2>"  ## Ignores non-digit inside tags

    match = re.search(pattern, string)
    if match:
        n2 = int(match.group(1))
    return [n1, n2]


def parse_score_single(string):
    n1 = -1  ## -1 means unable to extract scores
    pattern = r"<score1>\D*(\d+(\.\d+)?|10)\D*</score1>"  ## Ignores non-digit inside tags

    match = re.search(pattern, string)
    if match:
        n1 = int(match.group(1))
    return n1


def parse_preference_score(string):
    # Helpfulness and Accuracy: <preference> Answer2 < / preference >
    # Writing Style and Presentation: < preference > Answer 2 < / preference >
    # Safety: < preference > Answer 2 < / preference >
    string = string.replace(' ', '')
    scores = [-1, -1, -1]
    criteria = ['Helpfulness and Accuracy', 'Writing Style and Presentation', 'Safety']
    pattern = r"<preference>(Answer \d)</preference>"

    for i in range(len(criteria)):
        match_pattern = f"{criteria[i]}: {pattern}"
        print(match_pattern)
        match = re.search(match_pattern, string)
        print(match)
        print("-----")
        if match:
            n1 = int(match.group(1).split()[-1])
            scores[i] = n1
    return scores
